on scc:


module load python3/3.10.12
module load pytorch/1.13.1  # or another version that fits your needs
source venv/bin/activate

lets try python3/3.10.12 ##this is looking good so far....




i need a resume to load latest and train starting there.

i want to do param counts when i load the model.


25 ish minutes per epoch with batch 100, consec 20 (so batch 2k for trainer) and 4 workers.

with 8 workers (this was 4 cpus) i get: 15 ish yo!!!
okay.  4 processers, 8 workers.  submitting.


what if you slowly decreased the lr.  look at how it starts and ends the epoch.
epoch_initial_loss = Li, epech_terminal_loss = Lt

Li will usually be higher than Lt.  Lt is often slightly lower than the next Li.  its like, intra epoch overfitting.
but if the lr is right then sometimes what it learns is good for the next stuff.  well, that atleast is extrapolatable data
extrapolatable learning is when a prediction on one batch is good for the data that follows it.  its random, so statistically there is
a good chance (correlation of causation?) that that learning was particularly good.  so, how do you exploit that? speed up the lr? lower it?
does that mean we like the neighborhood so slow down to enjoy it?
or raise it? I don't think it is raise it i think it is lower it.   so if you ever get one where the lr goes to zero, well you fucking did it
i wanna try this shit.
give it a max bound? oooh, or do you random? choose a lr at random in a range.
 if the proximal is good, lower the lr a tiny bit if Lt is higher than Li on the next measurement.
 if it isn't, sample with that rate as the mean. and have the sd decrease proportionally to the lr as well
 .  so if it goes up it fluctuates
 but if it goes down, it means you are learning stuff that is applicable.
 and if you get them all it goes to zero.
 will this let you get to a global? no it would be a local.
 so i guess we see what shape the loss landscape is.  did i make one that has one global and no other local?

 i like this quite a bit


frechet audio distance.
fad score? works on a group, take some real audio recordings, and put it through a network,
compare the distance.
singleton noise ratios? peak signal to noise ratio?


fid inception distance?



9/24
i want a scheduler and a beefy model.  I am getting variation.  it is generation more noise for longer sequences, its not intelligible, but some i hear like "voice"

scheduler is easiest.
oh, that was scc 1.

model 1 is the bigger guy.
running val:
model 1 sucked.  only got 14 epcohs.   how much time did i request?


decoder is built, untested

skip connections might be a good idea.  especially in the decoder.  what if each imput embedding was the same dim until the output stack.
that is a good later idea i think

imagine residuals in the encoder as well.  with the sum, also, the time layers being shared across layers is interesting.

swiss army local worked.  now beef it up baby

next step might be --resume functionality.

scc trial 2 has some different sounding stuff, but it aint english.  i can hear cadence to a large degree.

model 1 looks broken.  tragic.

it also didn't finish.  up the times yo.

some of these seem way slow.  mess with num workers i think

ok, i just looked at my code again and realized i was only summing after the initial layer in the encoder, and the final.  so fixed that.

running 4 of my configs right now.  i want to see training again.

messed with the summing, see if that helps, but didn't alone
the final relu on the mse output stack i think caused issues.  on my single digit set fixing that made it spit out real looking predictions, not all zero

train the encoder slower? it is getting way more data per forward pass, so yeah, slow it down?

the duplication? look at those gradients.  i bet they get way big.  i may need to like, divide by the max len each time.
see if they are zeroing like i expect too.


from swissarmy import SeqModel
config = {
    't_seq_bits': seq_bits,  # Example value for the input bit size
    't_seq_len': seq_max_len,    # Example value for the sequence length
    't_bits': bits,      # Example value for the bits used in the decoder

    'encoder': {
        't_layer_dim': 0,               # Example hidden layer dimension for encoder
        't_num_layers': 0,                # Example number of layers in the encoder's initial layer
        'fc_layers': 4,                   # Example number of fully connected layers in the encoder
        'encoder_layers': 2,              # Example number of encoder layers
        'one_hot_vocab_len': 10,          # Vocabulary size for one-hot encoding
        'one_hot_embedding_dim': 10       # Embedding dimension for one-hot encoding
    },

    'decoder': {
        't_layer_dim': 0,                # Example hidden layer dimension for decoder
        't_num_layers': 0,                # Example number of layers in the decoder's initial layer
        'fc_layers': 4,                   # Example number of fully connected layers in the decoder
        'decoder_layers': 1                # Example number of decoder layers
    },

    'output': {
        'mse_output_layers': 2,           # Number of layers in the MSE output head
        'mse_dim': 64,                     # Hidden dimension for the MSE output head
        'bce_output_layers': 2,            # Number of layers in the BCE output head
        'bce_dim': 64                      # Hidden dimension for the BCE output head
    }
}

That is working with swiss army.  "working", it is training.  i need to add my scheduler i think but i want it more hands on
i am thinking about doing it with a gpu interactively

does it make sense to have the bce portion off of the encoder? interesting.  it might actually.  for training purposes

the earliest layer gradients are increasing??? no, that was very biased.  looked at more and that wasn't the case
they are however way low.  not sure how the repeat is operating on them

turning up the encoder layers maybe helped training stability? messing wiht that

mvoing the notebook onto the scc
collapsed again.  weird


