on scc:


module load python3/3.10.12
module load pytorch/1.13.1  # or another version that fits your needs
source venv/bin/activate

lets try python3/3.10.12 ##this is looking good so far....




i need a resume to load latest and train starting there.

i want to do param counts when i load the model.


25 ish minutes per epoch with batch 100, consec 20 (so batch 2k for trainer) and 4 workers.

with 8 workers (this was 4 cpus) i get: 15 ish yo!!!
okay.  4 processers, 8 workers.  submitting.


what if you slowly decreased the lr.  look at how it starts and ends the epoch.
epoch_initial_loss = Li, epech_terminal_loss = Lt

Li will usually be higher than Lt.  Lt is often slightly lower than the next Li.  its like, intra epoch overfitting.
but if the lr is right then sometimes what it learns is good for the next stuff.  well, that atleast is extrapolatable data
extrapolatable learning is when a prediction on one batch is good for the data that follows it.  its random, so statistically there is
a good chance (correlation of causation?) that that learning was particularly good.  so, how do you exploit that? speed up the lr? lower it?
does that mean we like the neighborhood so slow down to enjoy it?
or raise it? I don't think it is raise it i think it is lower it.   so if you ever get one where the lr goes to zero, well you fucking did it
i wanna try this shit.
give it a max bound? oooh, or do you random? choose a lr at random in a range.
 if the proximal is good, lower the lr a tiny bit if Lt is higher than Li on the next measurement.
 if it isn't, sample with that rate as the mean. and have the sd decrease proportionally to the lr as well
 .  so if it goes up it fluctuates
 but if it goes down, it means you are learning stuff that is applicable.
 and if you get them all it goes to zero.
 will this let you get to a global? no it would be a local.
 so i guess we see what shape the loss landscape is.  did i make one that has one global and no other local?

 i like this quite a bit


frechet audio distance.
fad score? works on a group, take some real audio recordings, and put it through a network,
compare the distance.
singleton noise ratios? peak signal to noise ratio?


fid inception distance?



9/24
i want a scheduler and a beefy model.  I am getting variation.  it is generation more noise for longer sequences, its not intelligible, but some i hear like "voice"

scheduler is easiest.
oh, that was scc 1.

model 1 is the bigger guy.
running val:
model 1 sucked.  only got 14 epcohs.   how much time did i request?


decoder is built, untested

skip connections might be a good idea.  especially in the decoder.  what if each imput embedding was the same dim until the output stack.
that is a good later idea i think

imagine residuals in the encoder as well.  with the sum, also, the time layers being shared across layers is interesting.

swiss army local worked.  now beef it up baby

next step might be --resume functionality.

scc trial 2 has some different sounding stuff, but it aint english.  i can hear cadence to a large degree.

model 1 looks broken.  tragic.

it also didn't finish.  up the times yo.

some of these seem way slow.  mess with num workers i think

ok, i just looked at my code again and realized i was only summing after the initial layer in the encoder, and the final.  so fixed that.

running 4 of my configs right now.  i want to see training again.

messed with the summing, see if that helps, but didn't alone
the final relu on the mse output stack i think caused issues.  on my single digit set fixing that made it spit out real looking predictions, not all zero

train the encoder slower? it is getting way more data per forward pass, so yeah, slow it down?

the duplication? look at those gradients.  i bet they get way big.  i may need to like, divide by the max len each time.
see if they are zeroing like i expect too.


from swissarmy import SeqModel
config = {
    't_seq_bits': seq_bits,  # Example value for the input bit size
    't_seq_len': seq_max_len,    # Example value for the sequence length
    't_bits': bits,      # Example value for the bits used in the decoder

    'encoder': {
        't_layer_dim': 0,               # Example hidden layer dimension for encoder
        't_num_layers': 0,                # Example number of layers in the encoder's initial layer
        'fc_layers': 4,                   # Example number of fully connected layers in the encoder
        'encoder_layers': 2,              # Example number of encoder layers
        'one_hot_vocab_len': 10,          # Vocabulary size for one-hot encoding
        'one_hot_embedding_dim': 10       # Embedding dimension for one-hot encoding
    },

    'decoder': {
        't_layer_dim': 0,                # Example hidden layer dimension for decoder
        't_num_layers': 0,                # Example number of layers in the decoder's initial layer
        'fc_layers': 4,                   # Example number of fully connected layers in the decoder
        'decoder_layers': 1                # Example number of decoder layers
    },

    'output': {
        'mse_output_layers': 2,           # Number of layers in the MSE output head
        'mse_dim': 64,                     # Hidden dimension for the MSE output head
        'bce_output_layers': 2,            # Number of layers in the BCE output head
        'bce_dim': 64                      # Hidden dimension for the BCE output head
    }
}

That is working with swiss army.  "working", it is training.  i need to add my scheduler i think but i want it more hands on
i am thinking about doing it with a gpu interactively

does it make sense to have the bce portion off of the encoder? interesting.  it might actually.  for training purposes

the earliest layer gradients are increasing??? no, that was very biased.  looked at more and that wasn't the case
they are however way low.  not sure how the repeat is operating on them

turning up the encoder layers maybe helped training stability? messing wiht that

mvoing the notebook onto the scc
collapsed again.  weird


module load python3/3.10.12
module load pytorch/1.13.1

trying to get the venv working in interactive.
https://www.bu.edu/tech/support/research/software-and-programming/common-languages/python/python-editing/jupyter/

works!!! load both modules and the venv_nb when you requesst in interacti e

got it
i had to mess with my loss functions to put them on device, but it was minor and yeah.
request more cpus tomorrow

i think i want to set up swiss army small and see what can learn.  what lrs, what configs.
those should run quick too.
good work today.  you did some solid work.
i know you don't feel like it sometimes, but you got stuff done, you planned, you executed.  we dispelled some unkonw?
ikd.  i named some unknown and made tools to investigate it.

with my 2 digit thing, i might just start testing lots of architectures.
res blocks
hyper params.
i think i am nas now
not neural, just as

digit_dataset in CSN (local) has multiple digits.


kulis meeting 10/3
norm layer?
batch norm.  keeps gradients bounded

shouldnt vanish until you hit

look at all the layers.

good to identify end backwards, if its summation, how to deal with that?

do i get the "vanishing gradients" with 2?

t encoding, if you scaled that down, what would be the impact.

from gradient perspective, normalize the inputs.  that is a non normalized input.
what is the range.  if there is a scaling issue.

in a transformer we have a positional embedding, but somehow it works.  learned embedding.  works just as well.

4 workers on my scc notebook made it lightning fast yo

the encoder is going to go a lot of the complex work.  the decoder i have shown if it is complex enough can make audio.

so, what does that imply? big encoder? should one learn faster or slower? i guess faster means more reactive? i think the encoder
learns faster?

wrote this in the swiss army model doc, but one of the big models (lots of encoder and decoder layers, lots of fc layers, tall output head)
when inited output all zeros.  my guess is that isn't gonna train at all.  look into init.

to train long range stuff, could you just do nonsense words at the beginning and then actual logical sentences at the end? like
i think it would be part of the training set up, i think this is way down the road, but just a thought.
you want to be able to say the sentence at any time in t so if you did oh sequence in, audio out, "quick brown fox" but with random
words before it, random after, bla bla.

oooh, might make sense to move the digits around. like, when you train.  that is a solid idea now i think about it.  make it predict 03
but with a few extra blank spaces at the beginning.  will mess

i really don't want to make it dependent on the previous time step.  buuut, what if it just took in 2 ts.  that could be interesting.
will consider. i like this.  i think i might be able to do it sort of for free with my dataloader, becuase of my differences thing
if it has both can it learn time as directional? i'm really warming up to it.  echo chamber but yeah.


10/7
"good_big_new_t.pth" saved last friday.  has some good speach.  i am training it more, slowly, reducing lr.

so, it looks like the bce is super low, but what i am thinking is only having it train the bce stack.  it looks useless right now
i actually just turned it off.  lets just get speach
turned off mse too.

"digitssamodel_more.pth" is the training without bce and mse (after the initial bit) it doesn't seem to be much improved.

legendre t uses legendre polys, same model as above.  i have messed with losses tho.  will continue to do that

i am thinking about stitching the audio together.  it only has one audio file for each digit and stitches them.  chat with kulis.  i wonder
if that is what the thing is doing.  it learns 9 well becuase it is in the same spot.

the other is mess with the spot.  something that interacts in an embedding sort of way that can change the decoder embedding.
i have seen an embedding and my t make a long audio file.  what if it could learn to swap those out.  i might already have this with multiple decoders.
maybe that dim is too big? it has so much neuronal space that it isn't forced to learn dense information, it tries to just memorize?

from my sketch book, what can i condition t on.  before i use it can i pass it through some layers and make changes to t?
oooooh, that is interesting.  i need a model that can learn an additive thing to t and move it around.  then can i learn an audio
segment wherever it is in time? there are inflections and stuff too, how do i deal with those?
my 2 t inputs is a thought.
can you learn a function of t and seq embedding to add to itself (t) before concatenating with the imput emb? to move around in time? do this in the decoder?


of course vary stuff, but, lets try a small model, 2 encoder layers, and lets up the decoder layers?

Legendre T and 0.5 for both cdif and cdif_batch looks okay.  started at 0.001 for 5 epochs
ok, so my original t makes sure to start at the nyquist frequency, this goes the opposite direction.  i need more bits! it doesn't have the highest freq stuff

all 0
0.0001 looks like its doing something.



