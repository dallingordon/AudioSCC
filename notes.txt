on scc:


module load python3/3.10.12
module load pytorch/1.13.1  # or another version that fits your needs
source venv/bin/activate

lets try python3/3.10.12 ##this is looking good so far....




i need a resume to load latest and train starting there.

i want to do param counts when i load the model.


25 ish minutes per epoch with batch 100, consec 20 (so batch 2k for trainer) and 4 workers.

with 8 workers (this was 4 cpus) i get: 15 ish yo!!!
okay.  4 processers, 8 workers.  submitting.


what if you slowly decreased the lr.  look at how it starts and ends the epoch.
epoch_initial_loss = Li, epech_terminal_loss = Lt

Li will usually be higher than Lt.  Lt is often slightly lower than the next Li.  its like, intra epoch overfitting.
but if the lr is right then sometimes what it learns is good for the next stuff.  well, that atleast is extrapolatable data
extrapolatable learning is when a prediction on one batch is good for the data that follows it.  its random, so statistically there is
a good chance (correlation of causation?) that that learning was particularly good.  so, how do you exploit that? speed up the lr? lower it?
does that mean we like the neighborhood so slow down to enjoy it?
or raise it? I don't think it is raise it i think it is lower it.   so if you ever get one where the lr goes to zero, well you fucking did it
i wanna try this shit.
give it a max bound? oooh, or do you random? choose a lr at random in a range.
 if the proximal is good, lower the lr a tiny bit if Lt is higher than Li on the next measurement.
 if it isn't, sample with that rate as the mean. and have the sd decrease proportionally to the lr as well
 .  so if it goes up it fluctuates
 but if it goes down, it means you are learning stuff that is applicable.
 and if you get them all it goes to zero.
 will this let you get to a global? no it would be a local.
 so i guess we see what shape the loss landscape is.  did i make one that has one global and no other local?

 i like this quite a bit


frechet audio distance.
fad score? works on a group, take some real audio recordings, and put it through a network,
compare the distance.
singleton noise ratios? peak signal to noise ratio?


fid inception distance?



9/24
i want a scheduler and a beefy model.  I am getting variation.  it is generation more noise for longer sequences, its not intelligible, but some i hear like "voice"

scheduler is easiest.
oh, that was scc 1.

model 1 is the bigger guy.
running val:
model 1 sucked.  only got 14 epcohs.   how much time did i request?


decoder is built, untested

skip connections might be a good idea.  especially in the decoder.  what if each imput embedding was the same dim until the output stack.
that is a good later idea i think

imagine residuals in the encoder as well.  with the sum, also, the time layers being shared across layers is interesting.

swiss army local worked.  now beef it up baby

next step might be --resume functionality.

scc trial 2 has some different sounding stuff, but it aint english.  i can hear cadence to a large degree.

model 1 looks broken.  tragic.

it also didn't finish.  up the times yo.

some of these seem way slow.  mess with num workers i think

ok, i just looked at my code again and realized i was only summing after the initial layer in the encoder, and the final.  so fixed that.

running 4 of my configs right now.  i want to see training again.

