{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e320a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the directory containing your module\n",
    "sys.path.append(os.path.abspath('/projectnb/textconv/dgwave/AudioSCC/data'))\n",
    "sys.path.append(os.path.abspath('/projectnb/textconv/dgwave/AudioSCC/losses'))\n",
    "sys.path.append(os.path.abspath('/projectnb/textconv/dgwave/AudioSCC/models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bec28fd-ab6d-4215-a35a-2d681195f99f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dataloader_prev_preload import WaveformDatasetPreload\n",
    "from sampler_prev import RandomConsecutiveSampler\n",
    "from losses import ConsecutiveDifferenceHigherOrderLossBatch, ConsecutiveDifferenceHigherOrderLoss\n",
    "from swissarmy_prev import SeqModel\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec9ceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use the first available GPU\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU found, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efebe3c2-8ce3-46c1-ac76-05a8214a5f3a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sine_tensor(num_bits, length):\n",
    "    # Create an array of integers from 0 to length - 1\n",
    "    t = np.arange(length)\n",
    "    # Generate the sine waves for each bit\n",
    "    sine_tensor = np.zeros((length, num_bits))  # Initialize the tensor\n",
    "    \n",
    "    for i in range(num_bits):\n",
    "        frequency = (np.pi / (2 ** i))  # Calculate frequency based on the number of bits\n",
    "        sine_tensor[:, i] = np.cos(frequency * (t))  # Fill the tensor with sine values\n",
    "\n",
    "    return sine_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d117af1-8ee4-4578-9eac-936e89d0d57e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_bits = 21\n",
    "max_len = 100_000\n",
    "seq_bits = 4\n",
    "seq_max_len = 2\n",
    "pred_prev = 1\n",
    "directory = \"/projectnb/textconv/dgwave/AudioSCC/data/digits_two/\"\n",
    "terminal_pad = 11\n",
    "seq_vocab_len = 10\n",
    "\n",
    "# Sampler setup as before\n",
    "batch_size = 200\n",
    "consecutive_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "638eda3a-1bd4-4231-ba3d-49c7a4655907",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100000, 21])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "t_input = torch.tensor(generate_sine_tensor(num_bits,max_len)).float()\n",
    "print(t_input.shape)\n",
    "\n",
    "seq_t = torch.tensor(generate_sine_tensor(seq_bits,seq_max_len)).float()\n",
    "print(seq_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56cb86a2-a43c-4365-a5f1-3684461a3b18",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n",
       "          1.0000e+00,  1.0000e+00],\n",
       "        [-1.0000e+00,  6.1232e-17,  7.0711e-01,  ...,  1.0000e+00,\n",
       "          1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00, -1.0000e+00,  6.1232e-17,  ...,  1.0000e+00,\n",
       "          1.0000e+00,  1.0000e+00],\n",
       "        ...,\n",
       "        [-1.0000e+00,  9.2903e-12, -7.0711e-01,  ...,  3.6386e-01,\n",
       "          8.2579e-01,  9.5546e-01],\n",
       "        [ 1.0000e+00, -1.0000e+00, -5.9268e-12,  ...,  3.6385e-01,\n",
       "          8.2579e-01,  9.5545e-01],\n",
       "        [-1.0000e+00, -1.4417e-11,  7.0711e-01,  ...,  3.6384e-01,\n",
       "          8.2578e-01,  9.5545e-01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23d19853-d5b9-4caf-9e8e-a0167ba150b5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [-1.0000e+00,  6.1232e-17,  7.0711e-01,  9.2388e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1980676f-4a26-468c-bba3-7d14cb894683",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = WaveformDatasetPreload(directory, t_input, max_len, terminal_pad, seq_vocab_len, seq_max_len, seq_t,pred_prev)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "sampler = RandomConsecutiveSampler(dataset, batch_size, consecutive_size)\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(dataset, batch_sampler=sampler, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "378851b8-8cff-4b44-93ae-0c4598979403",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27424])\n",
      "torch.Size([1, 27424, 1])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.wav_data_list[0].shape)\n",
    "print(dataset.pred_data_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2984f-1270-46ee-a372-ba228a569f42",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "\n",
    "for batch in dataloader:\n",
    "    wav_data, t_step, target, file_idx, seq_inputs, prev_target = batch #right now this wraps arround, just fyi.  not sure its a bad thing.\n",
    "\n",
    "    print(\"Waveform data:\", wav_data.shape)\n",
    "    print(\"Time step:\", t_step.shape)\n",
    "    print(\"Target tensor:\", target.shape)\n",
    "    print(\"File index:\", file_idx.shape)\n",
    "    print(\"File index:\", seq_inputs.shape)\n",
    "    print(\"Previous Predictions:\", prev_target.shape)\n",
    "    #print(wav_data)\n",
    "    #print(prev_target)\n",
    "    iteration = iteration + 1\n",
    "    if iteration > 10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47f6679b-16f1-4c3f-b363-92b2dc5e141a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    't_seq_bits': seq_bits,  # Example value for the input bit size\n",
    "    't_seq_len': seq_max_len,    # Example value for the sequence length\n",
    "    't_bits': num_bits,      # Example value for the bits used in the decoder\n",
    "\n",
    "    'encoder': {\n",
    "        't_layer_dim': 0,               # Example hidden layer dimension for encoder\n",
    "        't_num_layers': 0,                # Example number of layers in the encoder's initial layer\n",
    "        'fc_layers': 5,                   # Example number of fully connected layers in the encoder\n",
    "        'encoder_layers': 2,              # Example number of encoder layers\n",
    "        'one_hot_vocab_len': 10,          # Vocabulary size for one-hot encoding\n",
    "        'one_hot_embedding_dim': 100       # Embedding dimension for one-hot encoding\n",
    "    },\n",
    "\n",
    "    'decoder': {\n",
    "        't_layer_dim': 0,                # Example hidden layer dimension for decoder\n",
    "        't_num_layers': 0,                # Example number of layers in the decoder's initial layer\n",
    "        'fc_layers': 5,                   # Example number of fully connected layers in the decoder\n",
    "        'decoder_layers': 2,                # Example number of decoder layers\n",
    "        'pred_prev_dim': pred_prev\n",
    "    },\n",
    "\n",
    "    'output': {\n",
    "        'mse_output_layers': 4,           # Number of layers in the MSE output head\n",
    "        'mse_dim': 64,                     # Hidden dimension for the MSE output head\n",
    "        'bce_output_layers': 4,            # Number of layers in the BCE output head\n",
    "        'bce_dim': 64                      # Hidden dimension for the BCE output head\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861688d4-3ae0-425d-a636-299aa6a44a59",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = SeqModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a159c23-a767-47db-8b6d-4504d3d8c511",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68acd0b6-4bed-4fc9-bd9e-adc904bd42f1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 5623it [03:20, 28.06it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 MSE: 0.005035 BCE: 0.000813 CDIF: 0.005167  Total Loss: 0.02505695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 5623it [03:18, 28.38it/s]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 MSE: 0.006979 BCE: 0.000285 CDIF: 0.000229  Total Loss: 0.03910793\n",
      "all done sweetheart <3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "prev_target_list = []\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "bce_loss_fn = nn.BCELoss()\n",
    "cdifb_loss = ConsecutiveDifferenceHigherOrderLossBatch(consecutive_size,order=3)\n",
    "cdif_loss = ConsecutiveDifferenceHigherOrderLoss(consecutive_size,order=3)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    #for batch in dataloader:\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        wav_data, t_step, target, file_idx, seq_inputs, prev_target = batch\n",
    "        wav_data = wav_data.to(device)\n",
    "        t_step = t_step.to(device)\n",
    "        target = target.to(device)\n",
    "        file_idx = file_idx.to(device)\n",
    "        seq_inputs = seq_inputs.to(device)\n",
    "        prev_target = prev_target.to(device) ##add noise here\n",
    "        bce_output, mse_output = model(seq_inputs,file_idx,t_step,prev_target)\n",
    "        # Compute losses\n",
    "        mse_loss = mse_loss_fn(mse_output*target, wav_data)  # Assuming the target is for MSE # is this accomplishing what i want?\n",
    "        bce_loss = bce_loss_fn(bce_output, target)  # Assuming the target is for BCE\n",
    "        cdif = cdif_loss(mse_output*target, wav_data)\n",
    "        #bc = bc_loss(outputs, targets)\n",
    "        cdif_b = cdifb_loss(mse_output*target, wav_data)\n",
    "        \n",
    "        \n",
    "        # Combine losses (you can weight them if needed)\n",
    "        total_loss = 0.1*mse_loss + 0.01*bce_loss + 0.8*cdif  + 0.5*cdif_b #mess with mse\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Print progress for each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} MSE: {mse_loss.item():.6f} BCE: {bce_loss.item():.6f} CDIF: {cdif.item():.6f}  Total Loss: {total_loss.item():.8f}\")\n",
    "    torch.save(model, \"new_model_2.pth\")\n",
    "print(\"all done sweetheart <3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cbdc714d-bcb3-4d4b-99f6-958f27675680",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeqModel(\n",
       "  (encoder): SeqEncoder(\n",
       "    (initial_layer): SwissArmyLayer(\n",
       "      (t_layers): ModuleList()\n",
       "      (embedding): Embedding(11, 100, padding_idx=10)\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=104, out_features=104, bias=True)\n",
       "        (1): Linear(in_features=104, out_features=104, bias=True)\n",
       "        (2): Linear(in_features=104, out_features=104, bias=True)\n",
       "        (3): Linear(in_features=104, out_features=104, bias=True)\n",
       "        (4): Linear(in_features=104, out_features=104, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0): SwissArmyLayer(\n",
       "        (t_layers): ModuleList()\n",
       "        (embedding): Embedding(11, 100, padding_idx=10)\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=208, out_features=208, bias=True)\n",
       "          (1): Linear(in_features=208, out_features=208, bias=True)\n",
       "          (2): Linear(in_features=208, out_features=208, bias=True)\n",
       "          (3): Linear(in_features=208, out_features=208, bias=True)\n",
       "          (4): Linear(in_features=208, out_features=208, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SwissArmyLayer(\n",
       "        (t_layers): ModuleList()\n",
       "        (embedding): Embedding(11, 100, padding_idx=10)\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=312, out_features=312, bias=True)\n",
       "          (1): Linear(in_features=312, out_features=312, bias=True)\n",
       "          (2): Linear(in_features=312, out_features=312, bias=True)\n",
       "          (3): Linear(in_features=312, out_features=312, bias=True)\n",
       "          (4): Linear(in_features=312, out_features=312, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): SeqDecoder(\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0): SwissArmyLayer(\n",
       "        (t_layers): ModuleList()\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=334, out_features=334, bias=True)\n",
       "          (1): Linear(in_features=334, out_features=334, bias=True)\n",
       "          (2): Linear(in_features=334, out_features=334, bias=True)\n",
       "          (3): Linear(in_features=334, out_features=334, bias=True)\n",
       "          (4): Linear(in_features=334, out_features=334, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SwissArmyLayer(\n",
       "        (t_layers): ModuleList()\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=356, out_features=356, bias=True)\n",
       "          (1): Linear(in_features=356, out_features=356, bias=True)\n",
       "          (2): Linear(in_features=356, out_features=356, bias=True)\n",
       "          (3): Linear(in_features=356, out_features=356, bias=True)\n",
       "          (4): Linear(in_features=356, out_features=356, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mse_head): Sequential(\n",
       "    (0): Linear(in_features=356, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (bce_head): Sequential(\n",
       "    (0): Linear(in_features=356, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (9): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##i need inference and then a noise term for the training prev_pred input.  \n",
    "model = torch.load(\"new_model_2.pth\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ffa4a0fd-e789-4a14-a255-01d7b48fd536",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sequential_inf(digits,pred_prev,seq_t,t_input,model,device,num_inferences = 20_000,):\n",
    "    model.eval()\n",
    "    #encode the sequence:\n",
    "    file = [int(char) for char in digits] \n",
    "    file = file + [10] * (2 - len(file))\n",
    "    file = torch.tensor(file)\n",
    "    file = file.unsqueeze(0)\n",
    "    file = file.to(device)\n",
    "    #encoder t\n",
    "    input_seq_t = seq_t\n",
    "    input_seq_t = input_seq_t.unsqueeze(0)\n",
    "    input_seq_t[len(digits):] = 0\n",
    "    input_seq_t = input_seq_t.to(device)\n",
    "    #initial prev\n",
    "    prev_target = torch.tensor([[0.0]*pred_prev]) \n",
    "    prev_target = prev_target.to(device)\n",
    "    \n",
    "    mse_output = torch.zeros(num_inferences, 1).to(device)  # Shape (batch, 1)\n",
    "    bce_output = torch.zeros(num_inferences, 1).to(device) \n",
    "    \n",
    "    for i in range(num_inferences):\n",
    "        #print(i)\n",
    "        t_input_eval = t_input[i].to(device)\n",
    "        t_input_eval = t_input_eval.unsqueeze(0)\n",
    "        \n",
    "        cont_output, wave_output = model(input_seq_t,file,t_input_eval,prev_target)\n",
    "        \n",
    "        mse_output[i, 0] = wave_output.item()  # Assuming wave_output is a tensor of shape (1, 1)\n",
    "        bce_output[i, 0] = cont_output.item() \n",
    "        \n",
    "        prev_target = torch.cat((prev_target,wave_output),dim = -1)\n",
    "        prev_target = prev_target[:,1:]\n",
    "\n",
    "    return bce_output, mse_output  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2eb58971-6449-4300-b20e-b1d18d665beb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.io import wavfile\n",
    "file_path = '/projectnb/textconv/dgwave/AudioSCC/data/data_files/011.wav'\n",
    "sample_rate, data_test = wavfile.read(file_path)\n",
    "sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b4622-2dee-495b-917a-e107ac67c602",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b,m = sequential_inf(\"01\",pred_prev,seq_t,t_input,model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ed204-8fb0-486e-8591-a7c3b784c334",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e405a9e-cb7c-4bc4-8fac-15c86e44e6bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_outputs(bce_output, mse_output):\n",
    "    # Create a figure with two subplots\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "    # Plot BCE output\n",
    "    axs[0].plot(bce_output.cpu().numpy(), label='BCE Output', color='blue', linewidth=1)\n",
    "    axs[0].set_title('BCE Output over Inferences')\n",
    "    axs[0].set_xlabel('Inference Index')\n",
    "    axs[0].set_ylabel('BCE Value')\n",
    "    axs[0].grid()\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Plot MSE output\n",
    "    axs[1].plot(mse_output.cpu().numpy(), label='MSE Output', color='orange', linewidth=1)\n",
    "    axs[1].set_title('MSE Output over Inferences')\n",
    "    axs[1].set_xlabel('Inference Index')\n",
    "    axs[1].set_ylabel('MSE Value')\n",
    "    axs[1].grid()\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_outputs(b, m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6990f1-3c3a-4d79-b5eb-be32afcc3da0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "\n",
    "def tensor_to_wav(tensor, filename, sample_rate=44100,cut_off=-1 ):\n",
    "    # Convert tensor to numpy array and detach if needed\n",
    "    data = tensor.detach().cpu().numpy()[:cut_off]\n",
    "    # Normalize to the range [-1, 1]\n",
    "    #data = data / np.max(np.abs(data))\n",
    "\n",
    "    # Convert to 16-bit PCM format (values between -32768 and 32767)\n",
    "    data_int16 = np.int16(data * 32768)\n",
    "\n",
    "    # Write the .wav file\n",
    "    write(filename, sample_rate, data_int16)\n",
    "    print(f\"Saved as {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f3ed8-55f2-4153-8b18-0daa361bb3fc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Example usage with your model predictions (assuming predictions are in range -1 to 1):\n",
    "# predictions is the output tensor from the model\n",
    "tensor_to_wav(m, \"first_out_1.wav\",sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d4306c-1aa7-4215-81f2-07abb962c08c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "# Play the .wav file\n",
    "Audio(\"first_out_1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a4bd1-7e49-4755-b56d-a4669803fc04",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
